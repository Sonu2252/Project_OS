{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XJ9TdSr5K0A",
        "outputId": "4eb02f83-8f6b-4e04-ee48-37620a949b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.5.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2023.11.17)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.61.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-datasets numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset was generated synthetically using a sinusoidal function. The main signal is a sine wave, and random noise is added to simulate real-world variability.**\n",
        "\n",
        "**Features (X): The input features are created by windowing the time series. Each input feature is a subsequence of the time series, and the window size is set to 10.**\n",
        "**Target (y): The target variable is the next value in the time series following each window. In other words, y is the value immediately after the window.**"
      ],
      "metadata": {
        "id": "RiSaOe7nAXFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Small number of datasets"
      ],
      "metadata": {
        "id": "1ZBiWHUp7NBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets: 1000"
      ],
      "metadata": {
        "id": "WmPcjjAU7W8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Function to build and train the model\n",
        "def build_and_train_model(X, y, epochs=10, device='cpu'):\n",
        "    with tf.device(device):\n",
        "        model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y, epochs=epochs, validation_split=0.2)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "    return model, duration\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test, device='cpu'):\n",
        "    with tf.device(device):\n",
        "        start_time = time.time()\n",
        "        loss = model.evaluate(X_test, y_test)\n",
        "        predictions = model.predict(X_test)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "    return loss, predictions, duration\n",
        "\n",
        "# Function to calculate throughput\n",
        "def calculate_throughput(predictions, duration):\n",
        "    total_predictions = len(predictions)\n",
        "    throughput = total_predictions / duration\n",
        "    return throughput\n",
        "\n",
        "# Function to calculate latency\n",
        "def calculate_latency(duration, total_predictions):\n",
        "    latency = duration / total_predictions\n",
        "    return latency\n",
        "\n",
        "# Generate synthetic time series data\n",
        "def generate_synthetic_data():\n",
        "    np.random.seed(0)\n",
        "    data = np.sin(np.linspace(0, 100, 1000)) + 0.1 * np.random.randn(1000)\n",
        "    window_size = 10\n",
        "\n",
        "    X = np.array([data[i:i+window_size] for i in range(len(data) - window_size)])\n",
        "    y = data[window_size:]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Main program\n",
        "X, y = generate_synthetic_data()\n",
        "\n",
        "# Split into training and testing sets\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Run on CPU\n",
        "cpu_model, cpu_duration = build_and_train_model(X_train, y_train, epochs=10, device='cpu')\n",
        "cpu_loss, cpu_predictions, cpu_eval_duration = evaluate_model(cpu_model, X_test, y_test, device='cpu')\n",
        "\n",
        "# Calculate throughput and latency for CPU\n",
        "cpu_throughput = calculate_throughput(cpu_predictions, cpu_eval_duration)\n",
        "cpu_latency = calculate_latency(cpu_eval_duration, len(cpu_predictions))\n",
        "\n",
        "print(\"Results for CPU:\")\n",
        "print(f\"Training Duration: {cpu_duration:.2f} seconds\")\n",
        "print(f\"Test Loss: {cpu_loss:.4f}\")\n",
        "print(f\"Evaluation Duration: {cpu_eval_duration:.2f} seconds\")\n",
        "print(f\"Throughput: {cpu_throughput:.2f} predictions/second\")\n",
        "print(f\"Latency: {cpu_latency * 1000:.2f} milliseconds/prediction\\n\")\n",
        "\n",
        "# Run on GPU (assuming CUDA is available)\n",
        "try:\n",
        "    gpu_model, gpu_duration = build_and_train_model(X_train, y_train, epochs=10, device='gpu')\n",
        "    gpu_loss, gpu_predictions, gpu_eval_duration = evaluate_model(gpu_model, X_test, y_test, device='gpu')\n",
        "\n",
        "    # Calculate throughput and latency for GPU\n",
        "    gpu_throughput = calculate_throughput(gpu_predictions, gpu_eval_duration)\n",
        "    gpu_latency = calculate_latency(gpu_eval_duration, len(gpu_predictions))\n",
        "\n",
        "    print(\"Results for GPU:\")\n",
        "    print(f\"Training Duration: {gpu_duration:.2f} seconds\")\n",
        "    print(f\"Test Loss: {gpu_loss:.4f}\")\n",
        "    print(f\"Evaluation Duration: {gpu_eval_duration:.2f} seconds\")\n",
        "    print(f\"Throughput: {gpu_throughput:.2f} predictions/second\")\n",
        "    print(f\"Latency: {gpu_latency * 1000:.2f} milliseconds/prediction\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running on GPU: {e}\")\n",
        "    print(\"GPU execution not attempted due to an error.\")\n",
        "    gpu_duration = 0\n",
        "    gpu_eval_duration = 0\n",
        "    gpu_throughput = 0\n",
        "    gpu_latency = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBB0aC2k5MPQ",
        "outputId": "f9dde1b1-e3a9-4482-ed5c-bb3508d78166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 1s 10ms/step - loss: 0.1756 - val_loss: 0.1001\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.0367\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0284 - val_loss: 0.0202\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0184 - val_loss: 0.0166\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0161\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0164 - val_loss: 0.0161\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.0157\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0164 - val_loss: 0.0171\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0162 - val_loss: 0.0157\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0160 - val_loss: 0.0159\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "Results for CPU:\n",
            "Training Duration: 1.92 seconds\n",
            "Test Loss: 0.0129\n",
            "Evaluation Duration: 0.40 seconds\n",
            "Throughput: 496.47 predictions/second\n",
            "Latency: 2.01 milliseconds/prediction\n",
            "\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - 5s 9ms/step - loss: 0.1310 - val_loss: 0.0829\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0525 - val_loss: 0.0314\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.0235 - val_loss: 0.0186\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.0172\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0167\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.0171\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0163\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0161\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0158\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0158\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0134\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "Results for GPU:\n",
            "Training Duration: 10.65 seconds\n",
            "Test Loss: 0.0134\n",
            "Evaluation Duration: 0.21 seconds\n",
            "Throughput: 953.13 predictions/second\n",
            "Latency: 1.05 milliseconds/prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJxadFQi5sDk",
        "outputId": "51600387-304b-4ee6-be9b-f9504bf408a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(990, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MApC4GOG6dmX",
        "outputId": "63bad7da-0beb-4c02-aa62-6e9aa7f743f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(990,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Very Large Dataset = 1 Million"
      ],
      "metadata": {
        "id": "Aa_Z9HwR7SZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Function to build and train the model\n",
        "def build_and_train_model(X, y, epochs=10, device='cpu'):\n",
        "    with tf.device(device):\n",
        "        model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X, y, epochs=epochs, validation_split=0.2)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "    return model, duration\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test, device='cpu'):\n",
        "    with tf.device(device):\n",
        "        start_time = time.time()\n",
        "        loss = model.evaluate(X_test, y_test)\n",
        "        predictions = model.predict(X_test)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "    return loss, predictions, duration\n",
        "\n",
        "# Function to calculate throughput\n",
        "def calculate_throughput(predictions, duration):\n",
        "    total_predictions = len(predictions)\n",
        "    throughput = total_predictions / duration\n",
        "    return throughput\n",
        "\n",
        "# Function to calculate latency\n",
        "def calculate_latency(duration, total_predictions):\n",
        "    latency = duration / total_predictions\n",
        "    return latency\n",
        "\n",
        "# Generate synthetic time series data\n",
        "def generate_synthetic_data():\n",
        "    np.random.seed(0)\n",
        "    data = np.sin(np.linspace(0, 100, 1000000)) + 0.1 * np.random.randn(1000000)\n",
        "    window_size = 10\n",
        "\n",
        "    X = np.array([data[i:i+window_size] for i in range(len(data) - window_size)])\n",
        "    y = data[window_size:]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Main program\n",
        "X, y = generate_synthetic_data()\n",
        "\n",
        "# Split into training and testing sets\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Run on CPU\n",
        "cpu_model, cpu_duration = build_and_train_model(X_train, y_train, epochs=10, device='cpu')\n",
        "cpu_loss, cpu_predictions, cpu_eval_duration = evaluate_model(cpu_model, X_test, y_test, device='cpu')\n",
        "\n",
        "# Calculate throughput and latency for CPU\n",
        "cpu_throughput = calculate_throughput(cpu_predictions, cpu_eval_duration)\n",
        "cpu_latency = calculate_latency(cpu_eval_duration, len(cpu_predictions))\n",
        "\n",
        "print(\"Results for CPU:\")\n",
        "print(f\"Training Duration: {cpu_duration:.2f} seconds\")\n",
        "print(f\"Test Loss: {cpu_loss:.4f}\")\n",
        "print(f\"Evaluation Duration: {cpu_eval_duration:.2f} seconds\")\n",
        "print(f\"Throughput: {cpu_throughput:.2f} predictions/second\")\n",
        "print(f\"Latency: {cpu_latency * 1000:.2f} milliseconds/prediction\\n\")\n",
        "\n",
        "# Run on GPU (assuming CUDA is available)\n",
        "try:\n",
        "    gpu_model, gpu_duration = build_and_train_model(X_train, y_train, epochs=10, device='gpu')\n",
        "    gpu_loss, gpu_predictions, gpu_eval_duration = evaluate_model(gpu_model, X_test, y_test, device='gpu')\n",
        "\n",
        "    # Calculate throughput and latency for GPU\n",
        "    gpu_throughput = calculate_throughput(gpu_predictions, gpu_eval_duration)\n",
        "    gpu_latency = calculate_latency(gpu_eval_duration, len(gpu_predictions))\n",
        "\n",
        "    print(\"Results for GPU:\")\n",
        "    print(f\"Training Duration: {gpu_duration:.2f} seconds\")\n",
        "    print(f\"Test Loss: {gpu_loss:.4f}\")\n",
        "    print(f\"Evaluation Duration: {gpu_eval_duration:.2f} seconds\")\n",
        "    print(f\"Throughput: {gpu_throughput:.2f} predictions/second\")\n",
        "    print(f\"Latency: {gpu_latency * 1000:.2f} milliseconds/prediction\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running on GPU: {e}\")\n",
        "    print(\"GPU execution not attempted due to an error.\")\n",
        "    gpu_duration = 0\n",
        "    gpu_eval_duration = 0\n",
        "    gpu_throughput = 0\n",
        "    gpu_latency = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6alG-JF6eYt",
        "outputId": "fa35db3b-fbb4-4bca-93db-b38f5b829bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 52s 3ms/step - loss: 0.0115 - val_loss: 0.0111\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 51s 3ms/step - loss: 0.0111 - val_loss: 0.0114\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 46s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 45s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 46s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 47s 2ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 49s 2ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 50s 3ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 45s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 45s 2ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "6250/6250 [==============================] - 9s 1ms/step - loss: 0.0111\n",
            "6250/6250 [==============================] - 8s 1ms/step\n",
            "Results for CPU:\n",
            "Training Duration: 502.40 seconds\n",
            "Test Loss: 0.0111\n",
            "Evaluation Duration: 29.93 seconds\n",
            "Throughput: 6681.70 predictions/second\n",
            "Latency: 0.15 milliseconds/prediction\n",
            "\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 71s 3ms/step - loss: 0.0123 - val_loss: 0.0111\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 70s 3ms/step - loss: 0.0111 - val_loss: 0.0113\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0111 - val_loss: 0.0112\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "6250/6250 [==============================] - 14s 2ms/step - loss: 0.0110\n",
            "6250/6250 [==============================] - 11s 2ms/step\n",
            "Results for GPU:\n",
            "Training Duration: 682.44 seconds\n",
            "Test Loss: 0.0110\n",
            "Evaluation Duration: 28.44 seconds\n",
            "Throughput: 7032.98 predictions/second\n",
            "Latency: 0.14 milliseconds/prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language Processing in GPU vs CPU"
      ],
      "metadata": {
        "id": "Lt_ddNCiBKGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Twitter Sentiment Analysis"
      ],
      "metadata": {
        "id": "tU5eav_7BQZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# Load positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Assuming you want to use a balanced dataset, you can limit the number of tweets\n",
        "# and create labels for positive (1) and negative (0) sentiment\n",
        "positive_tweets = positive_tweets[:5000]\n",
        "negative_tweets = negative_tweets[:5000]\n",
        "\n",
        "# Create labels\n",
        "positive_labels = [1] * len(positive_tweets)\n",
        "negative_labels = [0] * len(negative_tweets)\n",
        "\n",
        "# Combine positive and negative tweets and labels\n",
        "tweets = positive_tweets + negative_tweets\n",
        "labels = positive_labels + negative_labels\n"
      ],
      "metadata": {
        "id": "LSt1ruVM7Aek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43b7d13-35f8-4506-d5e1-dcef541d1fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have tweets and labels from the Twitter sentiment analysis dataset\n",
        "# tweets and labels should be lists or arrays containing text and corresponding labels\n",
        "# You might need to perform additional preprocessing on the text data.\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and pad the text data\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = max(len(seq) for seq in X_train)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert labels to NumPy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on CPU\n",
        "start_time_cpu = time.time()\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "duration_cpu = time.time() - start_time_cpu\n",
        "\n",
        "# Evaluate the model on CPU\n",
        "start_time_cpu_eval = time.time()\n",
        "y_pred_cpu = (model.predict(X_test) > 0.5).astype(int)\n",
        "accuracy_cpu = accuracy_score(y_test, y_pred_cpu)\n",
        "duration_cpu_eval = time.time() - start_time_cpu_eval\n",
        "\n",
        "# Calculate throughput and latency for CPU\n",
        "total_predictions_cpu = len(y_test)\n",
        "throughput_cpu = total_predictions_cpu / duration_cpu_eval\n",
        "latency_cpu = duration_cpu_eval / total_predictions_cpu\n",
        "\n",
        "print(\"Results for CPU:\")\n",
        "print(f\"Training Duration: {duration_cpu:.2f} seconds\")\n",
        "print(f\"Test Accuracy: {accuracy_cpu:.4f}\")\n",
        "print(f\"Evaluation Duration: {duration_cpu_eval:.2f} seconds\")\n",
        "print(f\"Throughput: {throughput_cpu:.2f} predictions/second\")\n",
        "print(f\"Latency: {latency_cpu * 1000:.2f} milliseconds/prediction\\n\")\n",
        "\n",
        "# Train and evaluate the model on GPU (assuming CUDA is available)\n",
        "try:\n",
        "    with tf.device('gpu'):\n",
        "        start_time_gpu = time.time()\n",
        "        model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "        duration_gpu = time.time() - start_time_gpu\n",
        "\n",
        "        start_time_gpu_eval = time.time()\n",
        "        y_pred_gpu = (model.predict(X_test) > 0.5).astype(int)\n",
        "        accuracy_gpu = accuracy_score(y_test, y_pred_gpu)\n",
        "        duration_gpu_eval = time.time() - start_time_gpu_eval\n",
        "\n",
        "    # Calculate throughput and latency for GPU\n",
        "    total_predictions_gpu = len(y_test)\n",
        "    throughput_gpu = total_predictions_gpu / duration_gpu_eval\n",
        "    latency_gpu = duration_gpu_eval / total_predictions_gpu\n",
        "\n",
        "    print(\"Results for GPU:\")\n",
        "    print(f\"Training Duration: {duration_gpu:.2f} seconds\")\n",
        "    print(f\"Test Accuracy: {accuracy_gpu:.4f}\")\n",
        "    print(f\"Evaluation Duration: {duration_gpu_eval:.2f} seconds\")\n",
        "    print(f\"Throughput: {throughput_gpu:.2f} predictions/second\")\n",
        "    print(f\"Latency: {latency_gpu * 1000:.2f} milliseconds/prediction\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while running on GPU: {e}\")\n",
        "    print(\"GPU execution not attempted due to an error.\")\n",
        "    duration_gpu = 0\n",
        "    duration_gpu_eval = 0\n",
        "    throughput_gpu = 0\n",
        "    latency_gpu = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2iJp3WrELQT",
        "outputId": "5edd7006-28a9-4c07-97c3-7a0b2e3ea27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 20s 73ms/step - loss: 0.6627 - accuracy: 0.6212 - val_loss: 0.5737 - val_accuracy: 0.7685\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.4591 - accuracy: 0.8065 - val_loss: 0.4352 - val_accuracy: 0.8045\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.3084 - accuracy: 0.8850 - val_loss: 0.4271 - val_accuracy: 0.8045\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.2140 - accuracy: 0.9249 - val_loss: 0.4507 - val_accuracy: 0.7980\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.1453 - accuracy: 0.9532 - val_loss: 0.4696 - val_accuracy: 0.7995\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "Results for CPU:\n",
            "Training Duration: 29.10 seconds\n",
            "Test Accuracy: 0.7995\n",
            "Evaluation Duration: 0.39 seconds\n",
            "Throughput: 5140.60 predictions/second\n",
            "Latency: 0.19 milliseconds/prediction\n",
            "\n",
            "Epoch 1/5\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.1008 - accuracy: 0.9705 - val_loss: 0.5096 - val_accuracy: 0.7975\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0694 - accuracy: 0.9805 - val_loss: 0.5717 - val_accuracy: 0.7920\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0510 - accuracy: 0.9872 - val_loss: 0.6010 - val_accuracy: 0.7885\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0367 - accuracy: 0.9906 - val_loss: 0.6497 - val_accuracy: 0.7895\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0280 - accuracy: 0.9934 - val_loss: 0.6989 - val_accuracy: 0.7820\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "Results for GPU:\n",
            "Training Duration: 8.71 seconds\n",
            "Test Accuracy: 0.7820\n",
            "Evaluation Duration: 0.35 seconds\n",
            "Throughput: 5671.88 predictions/second\n",
            "Latency: 0.18 milliseconds/prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Low Latency task"
      ],
      "metadata": {
        "id": "K3Ugz-EUddVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import queue\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "\n",
        "# Function to simulate real-time data processing on CPU\n",
        "def process_data_cpu(data):\n",
        "    start_time = time.time()\n",
        "    print(f\"Processing data on CPU: {data}\")\n",
        "    # Simulate some processing time\n",
        "    time.sleep(0.1)\n",
        "    end_time = time.time()\n",
        "    print(f\"Data processing on CPU complete. Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Function to simulate real-time data processing on GPU using numba\n",
        "@jit\n",
        "def process_data_gpu(data):\n",
        "    start_time = time.time()\n",
        "    print(\"Processing data on GPU: %s\" % data)\n",
        "    # Simulate some processing time\n",
        "    time.sleep(0.1)\n",
        "    end_time = time.time()\n",
        "    print(\"Data processing on GPU complete. Time: %.2f seconds\" % (end_time - start_time))\n",
        "\n",
        "# Simulate real-time data arrival and processing on CPU\n",
        "def simulate_real_time_data_cpu(data_queue):\n",
        "    for i in range(10):\n",
        "        data_point = f\"Data Point {i}\"\n",
        "        print(f\"New data arrived: {data_point}\")\n",
        "        process_data_cpu(data_point)\n",
        "        time.sleep(0.05)\n",
        "\n",
        "# Simulate real-time data arrival and processing on GPU\n",
        "def simulate_real_time_data_gpu(data_queue):\n",
        "    for i in range(10):\n",
        "        data_point = f\"Data Point {i}\"\n",
        "        print(f\"New data arrived: {data_point}\")\n",
        "        process_data_gpu(data_point)\n",
        "        time.sleep(0.05)\n",
        "\n",
        "# Simulate real-time data arrival and processing on CPU\n",
        "data_queue_cpu = queue.Queue()\n",
        "start_time_cpu = time.time()\n",
        "simulate_real_time_data_cpu(data_queue_cpu)\n",
        "end_time_cpu = time.time()\n",
        "total_time_cpu = end_time_cpu - start_time_cpu\n",
        "print(f\"Total time on CPU: {total_time_cpu:.2f} seconds\")\n",
        "\n",
        "# Simulate real-time data arrival and processing on GPU\n",
        "data_queue_gpu = queue.Queue()\n",
        "start_time_gpu = time.time()\n",
        "simulate_real_time_data_gpu(data_queue_gpu)\n",
        "end_time_gpu = time.time()\n",
        "total_time_gpu = end_time_gpu - start_time_gpu\n",
        "print(f\"Total time on GPU: {total_time_gpu:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "_LujbMs6ESUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0987e69-bcd7-4029-871b-8f683e415e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-95b9c313105a>:17: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def process_data_gpu(data):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New data arrived: Data Point 0\n",
            "Processing data on CPU: Data Point 0\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 1\n",
            "Processing data on CPU: Data Point 1\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 2\n",
            "Processing data on CPU: Data Point 2\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 3\n",
            "Processing data on CPU: Data Point 3\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 4\n",
            "Processing data on CPU: Data Point 4\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 5\n",
            "Processing data on CPU: Data Point 5\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 6\n",
            "Processing data on CPU: Data Point 6\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 7\n",
            "Processing data on CPU: Data Point 7\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 8\n",
            "Processing data on CPU: Data Point 8\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 9\n",
            "Processing data on CPU: Data Point 9\n",
            "Data processing on CPU complete. Time: 0.10 seconds\n",
            "Total time on CPU: 1.51 seconds\n",
            "New data arrived: Data Point 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-95b9c313105a>:16: NumbaWarning: \n",
            "Compilation is falling back to object mode WITH looplifting enabled because Function \"process_data_gpu\" failed type inference due to: Unknown attribute 'time' of type Module(<module 'time' (built-in)>)\n",
            "\n",
            "File \"<ipython-input-5-95b9c313105a>\", line 18:\n",
            "def process_data_gpu(data):\n",
            "    start_time = time.time()\n",
            "    ^\n",
            "\n",
            "During: typing of get attribute at <ipython-input-5-95b9c313105a> (18)\n",
            "\n",
            "File \"<ipython-input-5-95b9c313105a>\", line 18:\n",
            "def process_data_gpu(data):\n",
            "    start_time = time.time()\n",
            "    ^\n",
            "\n",
            "  @jit\n",
            "/usr/local/lib/python3.10/dist-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"process_data_gpu\" was compiled in object mode without forceobj=True.\n",
            "\n",
            "File \"<ipython-input-5-95b9c313105a>\", line 17:\n",
            "@jit\n",
            "def process_data_gpu(data):\n",
            "^\n",
            "\n",
            "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
            "/usr/local/lib/python3.10/dist-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
            "\n",
            "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-5-95b9c313105a>\", line 17:\n",
            "@jit\n",
            "def process_data_gpu(data):\n",
            "^\n",
            "\n",
            "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data on GPU: Data Point 0\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 1\n",
            "Processing data on GPU: Data Point 1\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 2\n",
            "Processing data on GPU: Data Point 2\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 3\n",
            "Processing data on GPU: Data Point 3\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 4\n",
            "Processing data on GPU: Data Point 4\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 5\n",
            "Processing data on GPU: Data Point 5\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 6\n",
            "Processing data on GPU: Data Point 6\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 7\n",
            "Processing data on GPU: Data Point 7\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 8\n",
            "Processing data on GPU: Data Point 8\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "New data arrived: Data Point 9\n",
            "Processing data on GPU: Data Point 9\n",
            "Data processing on GPU complete. Time: 0.10 seconds\n",
            "Total time on GPU: 1.64 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZIB20oHdg2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}